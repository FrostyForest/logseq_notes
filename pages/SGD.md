- #神经网络 #优化算法 #深度学习
- ## 什么是SGD
	- 好的，我们来详细解释一下 **SGD (Stochastic Gradient Descent)**，中文通常翻译为 **随机梯度下降**。
	  
	  **简单来说：**
	  
	  SGD 是一种常用的**优化算法**，特别是在机器学习领域，用于**训练模型**（比如神经网络、线性回归、逻辑回归等）。它的核心目标是**找到一组模型参数（权重和偏置），使得模型的预测错误（损失函数的值）尽可能小**。
	  
	  **详细解释：**
	  
	  要理解 SGD，我们先把它拆开来看：
	  
	  1.  **Gradient Descent (梯度下降):**
	      *   想象你在一个山谷里，目标是尽快到达谷底（最低点）。
	      *   梯度（Gradient）告诉你当前位置**最陡峭的上坡方向**。
	      *   梯度下降（Gradient Descent）就是反其道而行之：每次都沿着**梯度的反方向**（最陡峭的下坡方向）走一小步。
	      *   重复这个过程，理论上你就能一步步接近并最终到达谷底（损失函数的最小值）。
	      *   在机器学习中，“位置”就是当前的**模型参数**，“山谷”就是**损失函数**（Loss Function）构成的多维空间，“谷底”就是**损失函数的最小值**，对应的参数就是我们想找的最佳参数。
	      *   **标准的梯度下降（也叫批量梯度下降 Batch Gradient Descent, BGD）** 在计算梯度时，需要用到**所有**的训练数据。对于非常大的数据集，这会非常耗时且占用大量内存。
	  
	  2.  **Stochastic (随机):**
	      *   这是 SGD 与标准梯度下降的关键区别。
	      *   "Stochastic" 意味着**随机性**或**概率性**。
	      *   SGD **不再使用全部数据**来计算梯度，而是**每次迭代（更新参数）时，只随机选择一个训练样本**来计算梯度，并立即根据这个样本的梯度更新模型参数。
	  
	  **SGD 的工作流程：**
	  
	  1.  **初始化：** 随机初始化模型的参数（权重 w 和偏置 b）。
	  2.  **迭代：** 重复以下步骤很多次（或者直到满足停止条件）：
	      *   **随机选择：** 从训练数据集中**随机**抽取**一个**样本 (x_i, y_i)。
	      *   **计算梯度：** 只使用这**一个样本**计算**损失函数**关于当前参数的梯度（∇L）。这个梯度是真实梯度的一个**有噪声的估计**。
	      *   **更新参数：** 沿着这个估计梯度的**反方向**更新参数：
	          `参数 = 参数 - 学习率 * 梯度`
	          （其中“学习率”是一个小的正数，控制每一步走多大）
	  3.  **结束：** 当达到预设的迭代次数或损失函数收敛到一定程度时停止。
	  
	  **Mini-Batch Gradient Descent (小批量梯度下降):**
	  
	  实际上，纯粹的 SGD（一次只用一个样本）因为梯度估计噪声太大，更新过程会非常不稳定。在实践中，更常用的是 **Mini-Batch Gradient Descent (MBGD)**：
	  
	  *   它介于 BGD 和 SGD 之间。
	  *   每次迭代时，不是用全部数据，也不是只用一个样本，而是随机选择一小部分数据（一个 **mini-batch**，比如 32、64、128 个样本）来计算梯度和更新参数。
	  *   **优点：**
	      *   比 BGD 快得多，内存占用少。
	      *   比纯粹的 SGD 更稳定，梯度估计更准确（因为在一个小批次上平均了）。
	      *   可以利用现代计算库和硬件（如 GPU）进行并行计算，效率很高。
	  
	  **注意：** 在很多现代深度学习框架和文献中，**人们经常直接用 "SGD" 来指代 Mini-Batch Gradient Descent**，因为它是实际中最常用的变种。
	  
	  **SGD (及其变种 MBGD) 的优缺点：**
	  
	  **优点：**
	  
	  1.  **速度快：** 每次更新参数的计算量小，尤其适合大规模数据集。
	  2.  **内存效率高：** 不需要一次性加载所有数据。
	  3.  **可能跳出局部最优：** 随机性（噪声）有时能帮助算法跳出损失函数的局部最小值，有更大机会找到全局最小值或更好的局部最小值。
	  4.  **适合在线学习：** 可以处理流式数据，来一个样本就更新一次模型。
	  
	  **缺点：**
	  
	  1.  **收敛波动大：** 更新方向比较随机，损失函数下降过程可能很不稳定，震荡较大。
	  2.  **可能不会精确收敛：** 由于噪声，可能在最小值附近震荡，而不是精确收敛到最小值。
	  3.  **对学习率敏感：** 学习率的选择非常重要，太大可能导致不收敛，太小则收敛过慢。通常需要使用学习率衰减策略。
	  4.  **对特征缩放敏感：** 如果不同特征的尺度差异很大，收敛会变慢，通常需要先进行特征归一化或标准化。
	  
	  **总结：**
	  
	  SGD 及其变种（尤其是 Mini-Batch SGD）是训练现代机器学习模型（特别是深度学习模型）最核心、最基础的优化算法之一。它通过每次使用部分数据（一个或一小批样本）来估计梯度并更新模型参数，以较低的计算成本实现了在大型数据集上快速寻找模型最优参数的目标。虽然它有一些缺点（如收敛波动），但通过各种改进（如动量法 Momentum、AdaGrad、RMSprop、Adam 等），这些问题在很大程度上得到了缓解。
-
- ## 评论
	- 个人对SGD如何起优化作用的例子：
		- 假设有参数权重w,输入数据x,输出结果y=wx,有ground truth z,损失l=|z-y|,假设x都大于0
			- 当y大于z,l=y-z,l对w求导得到梯度为x，
			- 当y小于z,l=z-y,l对w求导得到梯度为-x
				- $$w' = w - \alpha \frac{\mathrm{d}l}{\mathrm{d}w}= w - \alpha * (-x) $$
				- 可以发现，当y小于z时候，w是往增大的方向进行变化的
		- 关键是：**参数 = 参数 - 学习率 * 梯度**
			- 为什么是减而不是加，关键是**梯度**和**优化目标**是怎么定义的
				- 梯度向量 ∇f(P) 指向函数 f 在点 P 值增加最快的方向
				- 优化目标是使得损失尽可能小
				- 因此参数需要往梯度的反方向改进
-