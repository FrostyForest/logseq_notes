- #研究想法
- 利用 [[RWKV]] 建模action决策模型，选择哪些任务和方法做benchmark
  利用 预训练[[VLM]] 做信息提取器，利用 VQ-VAE将信息和动作映射到同一空间
- 针对精细任务，增加更多的传感器，对于每一个任务，采用端到端训练一个action model
  如何将不同的ACTION MODEL统一起来
- 如何使用[[Isaac Sim]]提高训练的效率和成功率
- 核心：利用多模态模型做高效的 [[Off Policy Learning]]
- 扩散生成也是一种自回归生成？不过它不是利用上下文的自回归，而是从模糊到清晰的自回归
- 预训练和端到端区别：预训练是首先利用无监督学习训练一个有损tokenizer，再利用该tokenizer训练下游模型
  端到端，所有参数一起进行学习。
- 什么东西能够用transformer做：token分类任务。
- 如何完成任务？训练类似[[思维链]]的东西，让模型学会选择正确的工具API.
- 为什么说大模型只能用于决策而不能完成具体的任务？具体例子就是人，人的决策系统和执行系统并不是一体的，它们是相互关联但又相对独立的两个系统。对于日常的习惯，我们通常可以不经过有意识的决策系统，而是通过潜意识的、自动化的过程来完成。
- 可以将问题转化为决策系统和动作系统。VLA模型通过综合信息决定该采取什么样的动作。问题是VLA模型决策的粒度是什么级别。
- 直接训练机器人能够完成各种任务，端到端的解决方案，我怎么觉得完全不可能呢，动作空间并不是个选择就能完成的，需要不断反馈调节。我更偏向于通过OFF POLICY强化学习训练能够完成各种任务的子模型再由大模型进行决策。
- 点云部分，能否使用gaussian splatting减少存储空间？能否实现动态建图？
	- gaussian存储语义
- 决策部分，如何将rag和cot结合进行合理规划同时能保证规划的可执行性，任务的某个环节失败了该如何处理
	- 对于如何实现test in time,主要还是蒙特拉罗搜索+微调+强化学习
	- 理想输出格式：
		- ```<action>action_type action_parameters<action\>```
- 如何解决物体的连续操作与场景图的维护问题
	- 一开始如果想获取某个物体的位置，可以通过查询场景图而无需遍历查找。但是一旦对某个物体进行操作后，它的位置发生变化，不可能马上去更新一遍场景图，这时候，我想采用的方法是，操作历史留痕，通过查询操作历史去尝试获得物体的位置。任务历史就是当天的短期记忆，场景图就是长期记忆。场景图每天维护一次差不多。一天之内的查询位置就通过任务历史。
- 如何进行规划
	- 法1：通过仿真环境生成大量数据并进行微调，逐token生成？看起来不可行
	- 法2：人工写文档，结合mcts和rag让模型去规划
	- 如何得到任务链：对llm的输出进行正则式匹配，按顺序执行。
- 如何保证规划的可执行性
	-
- 任务执行失败了怎么办
	- 任务执行过程中使用llm对过程进行记录以判断是否成功？
-
- 项目框架
	- 学习
	- 建图
	- 规划
	- 执行
-