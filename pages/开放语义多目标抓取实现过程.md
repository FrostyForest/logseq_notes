- #isaaclab #isaacsim #reinforcement_learning
- ## 环境
	- ```
	  Isaac-my_Lift-Cube-Franka-IK-Rel-v0
	  Isaac-my_Lift-Cube-Franka-v0
	  Isaac-Lift-Cube-Franka-v0
	  ```
	- ```
	  Isaac-Stack-Cube-Franka-IK-Rel-v0
	  ```
	-
- ## 想法
	- 直接通过强化学习直接学会可能吗
- ## 实现日志
	- ppo无法让模型学会lift
	- sac测试出了问题，机械臂消失了，疑似随机动作的问题？ppo正常
	- 妈呀，难道之前是数据量不够的原因吗，数据量上去了就训出来了
	- 当memorysize与rollout不相同，就会出现问题：机械臂位置出现问题
	- ### 20250522
		- 下一步计划实现开放语义抓取，即有不同颜色的cube,输入物体描述的embedding
		- 卡在了测试siglip2
	- ### 20250523
		- obs中需要包含任务文本和image的embedding
		- 如何实现多样物体的随机化？
		- 如何判定指令对应任务是否完成
	- ### 20250524
		- 如何将深度融入语义
		- 首先测试如何将siglip融入到工作流
		- manager_based obs self.concatenate_terms = Trues时的结果
			- {'policy': tensor([[-0.0495, -0.4073,  0.0774, -0.1531, -0.0029, -0.2197, -0.0316, -0.0232,
			           -0.0231, -1.5982,  2.1754,  2.1734,  1.7801, -2.5614, -2.6100,  2.6058,
			            0.1998,  0.2029,  0.5499, -0.2137,  0.0210,  0.5557, -0.1120,  0.3648,
			            1.0000,  0.0000,  0.0000,  0.0000, -0.0411, -0.0561,  0.8061,  0.3162,
			           -0.9827, -0.6996,  0.3813,  0.1613]], device='cuda:0')}
		- direct env obs结果
		  collapsed:: true
			- ```
			  {'policy': {'actions': tensor([[-0.5154, -0.0924,  0.5866,  0.3119,  0.5419,  0.2258,  0.4917, -0.7876,
			           -0.0724]], device='cuda:0'), 'rgb': tensor([[[[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4936, -2.4936, -2.4936]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4952, -2.4952, -2.4952]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4982, -2.4982, -2.4982]],
			  
			           ...,
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.4968, -2.4968, -2.4968],
			            [-2.4970, -2.4970, -2.4970],
			            [-2.4894, -2.4894, -2.4894]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.4975, -2.4975, -2.4975],
			            [-2.4998, -2.4998, -2.4998],
			            [-2.4962, -2.4962, -2.4962]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]]]], device='cuda:0'), 'rgb2': tensor([[[[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1209,  2.1209,  2.1209]],
			  
			           [[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1390,  2.1390,  2.1390]],
			  
			           [[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1453,  2.1453,  2.1453]],
			  
			           ...,
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]]]], device='cuda:0'), 'to_target': tensor([[ 0.6302,  0.0949, -0.2875]], device='cuda:0')}}
			  ```
		- manager based concatenate =False时候的obs结果
			- ```
			  {'policy': {'joint_pos': tensor([[ 0.0083,  0.0480,  0.0478, -0.0895, -0.0503, -0.0661, -0.1612,  0.0000,
			            0.0000]], device='cuda:0'), 'joint_vel': tensor([[-0.0299, -0.2751, -2.1749, -0.8127,  1.3739, -1.2388,  0.3891,  0.0066,
			            0.0025]], device='cuda:0'), 'object_position': tensor([[ 0.4847, -0.2363,  0.0210]], device='cuda:0'), 'target_object_position': tensor([[0.5022, 0.1148, 0.3830, 1.0000, 0.0000, 0.0000, 0.0000]],
			         device='cuda:0'), 'actions': tensor([[-0.4882, -0.3291, -0.8014,  0.3183,  0.9181, -0.1656, -0.2962,  0.4618]],
			         device='cuda:0')}}
			  ```
	- ### 20250525
		- obs采用dict方案，缺点，似乎不会兼容原来的训练方案，优点，结构更加清晰
			- 需要在env cfg中先设置好：
			  ```
			      observation_space = {
			          'rgb':[64,64,3],
			          'rgb2':[64,64,3],
			          'actions':9,
			          'to_target':3
			      }
			  ```
		- 剩下的问题是如何将siglip的工作流嵌入到环境中
	- ### 20250526
		- 还有reward要修改
			- 计划方案是同时计算对三个cube的奖励，再根据当时的目标对奖励进行加权平均
			- 根据target_idx生成(b,3),三个cube的奖励拼接成(b,3),直接点乘
		- 剩下就是修改模型结构来对齐obs的dict
			- joint_pos torch.Size([2, 9])
			  joint_vel torch.Size([2, 9])
			  object_position torch.Size([2, 9])
			  target_object_position torch.Size([2, 7])
			  actions torch.Size([2, 8])
			  image_feature torch.Size([2, 768])
			  text_feature torch.Size([2, 768])
		- self.observation_space有点问题
			- Box(-inf, inf, (36,), float32)
			- 正确的：
				- Dict('actions': Box(-inf, inf, (9,), float32), 'rgb': Box(-inf, inf, (64, 64, 3), float32), 'rgb2': Box(-inf, inf, (64, 64, 3), float32), 'to_target': Box(-inf, inf, (3,), float32))
	- ### 20250527
		- 我称我个人走的流派是基于agent框架的multi-model的具身智能
		- 尝试用一个大的动作模型去完成所有的任务是不现实的，task-obs-act空间并不像text和image一样能够统一表征
	- ### 20250528
		- 麻了，怀疑是给了多个位置的原因，给一个位置能lift,给多个位置就不能了？
		- 我靠见鬼了，突然学会了，然后reward暴跌？什么鬼原因，在10k步之后
			- 似乎是action_rate和joint_vel的reward的问题
	- ### 20250529
		- 尝试使用别的库进行训练
			- 换个训练脚本仍然会出现突然运动惩罚，其他的reward之后也出现大幅下跌
				- 很奇怪，使用了torch.clamp还是会爆表
				- 有毒啊，我tm都weight为0了还是会爆表？怪哉
				- 原来是CurrTerm的原因
			- 废了，他妈的只学会到某个位置之后就呆住不动了
			- 原版也会发生这种情况
			- 建议action_rate限定在-0.005之内，joint_vel限定在-0.005之内
			- 去掉camera能大量提高训练速度
		- 理想中的工作流
			- bot每日建模确定环境中各个物体的位置
			- 要想获取对应物体，bot先导航至相应位置
			- 然后根据指令、图像和物体位置pick相应物体
			- 然后导航回去
	- ### 20250530
		- 完整训练了36000step,然而并没有学会，中途似乎是有能lift的，然而最终连lift都无法稳定学习
	- ### 20250603
		- 为什么原版lift能训练成功，而我的无法训练成功，是pos的obs太多的原因吗
		- 越来越觉得，单纯的强化学习局限性太大了，必须要像大模型一样，监督学习训练一个底模再用强化学习去微调，那么应该如何设计模型的输出来适应两者呢？
			- 参考deepseek的方案？
		- 将obs砍成只获得yellow cube的pos，顺便修改了reward，环境数量猛增加到160
			- 训练成功了，我怀疑还是多个物体的pos互相干扰导致的
	- ### 20250604
		- 准备测试一下对其他物体能否lift
		- lift的通用输入应该是
			- 物体的pos和终点的pos
			- 至于有多个物体怎么办，我考虑的解决方案是，先pick再通过clip来评估相似性，如果不相似则逆转动作序列
	- ### 20250605
		- 目前已知给定物体位置能将物体抓取，能否不给定物体位置，而使用深度图来完成
		- 使用深度图的好处是，或许能够从多种物体中抓取对应物体？比如有多种饮料，使用深度图+语义分割，或许能抓对东西，如果光用位置，恐怕很难
			- 但是用到camera就训练起来很费劲
	- ### 20250609
		- 下一个目标是，仅利用初始位置和camera的rgb数据以及深度数据能否学会抓取
	- ### 20250610
		- 尝试魔改slam来兼容isaaclab
		- [[isaaclab兼容3dgs]]
	- ### 20250616
		- 如何方便地在isaacsim的standalone脚本来设置环境
		- 妈的原生的omni graph真是难用的像托shit,自己写ros2接收数据再传给differential controller
	- ### 20250620
		- 目前的3dgs slam难以训练大型场景
	- ### 20250630
		- 测试了，batchsize为40且无帧堆叠的情况下，reward能稳定增加，但是很慢，3wstep还没收敛
	- ### 20250701
		- 麻了，帧堆叠也没用，下一步测试加camera
		- 想法：已知深度和末端位姿，可以反推物体相对于机械臂的位置，再根据语义检索
		- 具体实现
			- 建立一个字典用来存储各个物体的位置信息
			- 将rgb图像喂给sam得到不同的方框
			- 筛选掉面积小于400且iou小于0.996的分割图片
				- 根据深度图和末端位姿计算出其在robot_root_frame下的位置
				- 将图片喂给clip得到embedding,embedding作为key,遍历dict的各个key,
					- 如果没有一个相似度>0.95，就新建一个索引
					- 如果有相似度>0.95，就覆盖原来的值
			- 将要寻找的物品的文本的embedding与dict中所有的key进行比较取相似度最高的key的值作为该物品的位置
	- ### 20250706
		- 基本完成整个系统，但是训练效果不尽如人意
		- 考虑进行对照实验，怀疑是图像数据干扰训练，考虑给出完美的位置数据训练看看
		- 给出完美的位置数据仍然训练比较慢，跟之前很像，卡在了0.025，什么原因
			- ik 控制问题？
				- 换了直接输出动作，训着训着莫名其妙全部reward都下降？
					- 推测是模型容量不够
				- 训得比直接控制要慢
			- 图像数据干扰？没用预训练模型导致收敛太慢
			- 训练框架有问题？
				- 很奇怪，换到另一个就很轻松训出来了？？？？？？？？？？
			- batch size 太小？当前为64
			- roll out 太小？当前为40
				- 寄了，方差增大
		- 我感觉是reward问题
			- /touching_object上升，/lifting_object反而下降
			- 奇怪，把touching_object去掉了还是训不好
		- 破案了，原来是模型参数量太小了我干,40 256 128 64 32才够，原来40 64 64 32 32
		- 调度器看起来有点问题
			- logstd设置为0时train不了，不知道什么原因
	- ### 20250708
		- 单纯想看下视觉信息对训练有无帮助
			- 25-07-08_01-26-35-649079_PPO，总体来讲确实比没有视觉要训练的好
		- 大概观察了一下，末端的camera位置似乎有点问题
		- 如何设计全新模型架构
			- 使用预训练模型处理rgb得到特征
			- 双camera
			- 使用无预训练的mobilenetv3_small_050.lamb_in1k处理深度图
	- ### 20250709
		- 考虑优化奖励函数
			- 统一用指数函数去封装reward,范围限制在0~1
		- 首先测试修改之后的效果
		- ？只有reaching的奖励在增长，lift奖励不增长，是weight为1太小了吗，改到15
		- 显然，增大了weight对lift的学习是立竿见影的，如何寻找一个最佳weight对学习很有好处
		- 估算reaching的极限distance在0.05左右，因此std设置为0.15比较合适
		- 训练好抓取黄色物体之后尝试鲁棒化object postilion然后再训练能分辨物体的模型
			- 因为之后要接入文本，所以observation需要包括文本的embedding
		- 看损失函数图像
			- https://www.desmos.com/calculator?lang=zh-CN
		- wtf,加了lift权重还是train不起来，是observation加太多了还是模型参数量太大了
		- 先训小模型，再蒸馏看看
	- ### 20250711
		- 准备测试不同优化器的效果
		- [[isaaclab模仿学习]]
		- 才发现，任何输入在输入到模型前需要经过RunningStandardScaler的处理，全乱套了
		- 准备测试用不要batchnorm处理输入的区别
			- 没加RunningStandardScaler和batchnorm的情况下完全训不起来
			- 加了batchnorm效果比没加略好，但还是远不如RunningStandardScaler
			- 先RunningStandardScaler再batchnorm效果好于不加RunningStandardScaler而差于不加batchnorm
		-
	- ### 20250712
		- 当前训练的最好的模型是25-07-11_17-18-21-810651_my_PPO
		- 狗肏的环境的observation变了，之前的state_preprocessor也用不上了，所以说preprocessor还是跟随模型才是比较优雅的，计划尝试用训练好的小模型去训练另一个优化架构的小模型看看效果如何
	- ### 20250713
		- 蒸馏学习和阻碍强化学习训练的问题基本都得到解决
		- 见鬼了，强化学习加载预训练的权重完全训不起来？一开始是能够完成任务的，结果到后面就烂掉了
			- 通过收紧ratio clip和kl threshold、减小学习率来约束策略更新的幅度
	- ### 20250714
		- 为什基于预训练的强化学习稳定拉垮
		- lfit_object的reward在涨，lift_ratio反而在跌
		- 蒸馏kl散度的稳定值大概在0.05左右
		- policy loss大概0.03，value loss大概0.7，显然policyloss的幅值似乎太小了
			- 体感还是要降低value model的更新幅度
			- 每次value loss一降低就是reward下降
			- policy scale增大明显表现要好些，value model和policy model分离的好处就是不存在policy model和value model梯度冲突的问题
			- 还是寄，根本train不起来卡死了原地打转
		- 总感觉，用mseloss去指导value model预测state value过于粗糙
			- 为什么，因为reward设置一般是很随意的，即使经过标准化仍然方差可能很大，让模型去预测这样一个东西比较困难，没有一种与学习目标相匹配的自然感
			- 怎么说呢，pick and place三个步骤，先接近，再抓取，再跟踪，你手动设置reward权重，可能会造成方差较大，有没有一种能够脱离手动设置权重影响的value计算方法呢？
		- 尝试将value model独立出来
	- ### 20250717
		- 实现了ratio clip、value clip、policy grad norm、rank loss和value loss权重等的自动调整
		- rankloss中的scale也需要自动调整以满足均值为0.5的要求
			- 放缩以满足x大于3的比例至少为1%
			- 偏置以满足sigmoid(x)的均值在0.5
		- 下一个目标是实现reward权重的自动调整
			- 直觉是越稀有的reward权重应该越大
			- 连续奖励与稀疏奖励，过程奖励与结果奖励
		- Policy Clipped Ratio=0.01直接烂掉了训不起来
	- ### 20250719
		- 尝试用cos对loss权重和梯度进行控制，结果是卵用没有，训练的更慢了
		- 尝试降低value clip比例并且不同cos 权重再进行训练