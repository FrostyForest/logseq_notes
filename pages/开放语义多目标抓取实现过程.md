- #isaaclab #isaacsim #reinforcement_learning
- ## 环境
	- ```
	  Isaac-my_Lift-Cube-Franka-IK-Rel-v0
	  Isaac-my_Lift-Cube-Franka-v0
	  Isaac-Lift-Cube-Franka-v0
	  ```
	- ```
	  Isaac-Stack-Cube-Franka-IK-Rel-v0
	  ```
	-
- ## 想法
	- 直接通过强化学习直接学会可能吗
- ## 实现日志
	- ppo无法让模型学会lift
	- sac测试出了问题，机械臂消失了，疑似随机动作的问题？ppo正常
	- 妈呀，难道之前是数据量不够的原因吗，数据量上去了就训出来了
	- 当memorysize与rollout不相同，就会出现问题：机械臂位置出现问题
	- ### 20250522
		- 下一步计划实现开放语义抓取，即有不同颜色的cube,输入物体描述的embedding
		- 卡在了测试siglip2
	- ### 20250523
		- obs中需要包含任务文本和image的embedding
		- 如何实现多样物体的随机化？
		- 如何判定指令对应任务是否完成
	- ### 20250524
		- 如何将深度融入语义
		- 首先测试如何将siglip融入到工作流
		- manager_based obs self.concatenate_terms = Trues时的结果
			- {'policy': tensor([[-0.0495, -0.4073,  0.0774, -0.1531, -0.0029, -0.2197, -0.0316, -0.0232,
			           -0.0231, -1.5982,  2.1754,  2.1734,  1.7801, -2.5614, -2.6100,  2.6058,
			            0.1998,  0.2029,  0.5499, -0.2137,  0.0210,  0.5557, -0.1120,  0.3648,
			            1.0000,  0.0000,  0.0000,  0.0000, -0.0411, -0.0561,  0.8061,  0.3162,
			           -0.9827, -0.6996,  0.3813,  0.1613]], device='cuda:0')}
		- direct env obs结果
		  collapsed:: true
			- ```
			  {'policy': {'actions': tensor([[-0.5154, -0.0924,  0.5866,  0.3119,  0.5419,  0.2258,  0.4917, -0.7876,
			           -0.0724]], device='cuda:0'), 'rgb': tensor([[[[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4936, -2.4936, -2.4936]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4952, -2.4952, -2.4952]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.4982, -2.4982, -2.4982]],
			  
			           ...,
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.4968, -2.4968, -2.4968],
			            [-2.4970, -2.4970, -2.4970],
			            [-2.4894, -2.4894, -2.4894]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.4975, -2.4975, -2.4975],
			            [-2.4998, -2.4998, -2.4998],
			            [-2.4962, -2.4962, -2.4962]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]]]], device='cuda:0'), 'rgb2': tensor([[[[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1209,  2.1209,  2.1209]],
			  
			           [[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1390,  2.1390,  2.1390]],
			  
			           [[ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            ...,
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1667,  2.1667,  2.1667],
			            [ 2.1453,  2.1453,  2.1453]],
			  
			           ...,
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]],
			  
			           [[-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            ...,
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000],
			            [-2.5000, -2.5000, -2.5000]]]], device='cuda:0'), 'to_target': tensor([[ 0.6302,  0.0949, -0.2875]], device='cuda:0')}}
			  ```
		- manager based concatenate =False时候的obs结果
			- ```
			  {'policy': {'joint_pos': tensor([[ 0.0083,  0.0480,  0.0478, -0.0895, -0.0503, -0.0661, -0.1612,  0.0000,
			            0.0000]], device='cuda:0'), 'joint_vel': tensor([[-0.0299, -0.2751, -2.1749, -0.8127,  1.3739, -1.2388,  0.3891,  0.0066,
			            0.0025]], device='cuda:0'), 'object_position': tensor([[ 0.4847, -0.2363,  0.0210]], device='cuda:0'), 'target_object_position': tensor([[0.5022, 0.1148, 0.3830, 1.0000, 0.0000, 0.0000, 0.0000]],
			         device='cuda:0'), 'actions': tensor([[-0.4882, -0.3291, -0.8014,  0.3183,  0.9181, -0.1656, -0.2962,  0.4618]],
			         device='cuda:0')}}
			  ```
	- ### 20250525
		- obs采用dict方案，缺点，似乎不会兼容原来的训练方案，优点，结构更加清晰
			- 需要在env cfg中先设置好：
			  ```
			      observation_space = {
			          'rgb':[64,64,3],
			          'rgb2':[64,64,3],
			          'actions':9,
			          'to_target':3
			      }
			  ```
		- 剩下的问题是如何将siglip的工作流嵌入到环境中
	- ### 20250526
		- 还有reward要修改
			- 计划方案是同时计算对三个cube的奖励，再根据当时的目标对奖励进行加权平均
			- 根据target_idx生成(b,3),三个cube的奖励拼接成(b,3),直接点乘
		- 剩下就是修改模型结构来对齐obs的dict
			- joint_pos torch.Size([2, 9])
			  joint_vel torch.Size([2, 9])
			  object_position torch.Size([2, 9])
			  target_object_position torch.Size([2, 7])
			  actions torch.Size([2, 8])
			  image_feature torch.Size([2, 768])
			  text_feature torch.Size([2, 768])
		- self.observation_space有点问题
			- Box(-inf, inf, (36,), float32)
			- 正确的：
				- Dict('actions': Box(-inf, inf, (9,), float32), 'rgb': Box(-inf, inf, (64, 64, 3), float32), 'rgb2': Box(-inf, inf, (64, 64, 3), float32), 'to_target': Box(-inf, inf, (3,), float32))
	- ### 20250527
		- 我称我个人走的流派是基于agent框架的multi-model的具身智能
		- 尝试用一个大的动作模型去完成所有的任务是不现实的，task-obs-act空间并不像text和image一样能够统一表征
	- ### 20250528
		- 麻了，怀疑是给了多个位置的原因，给一个位置能lift,给多个位置就不能了？
		- 我靠见鬼了，突然学会了，然后reward暴跌？什么鬼原因，在10k步之后
			- 似乎是action_rate和joint_vel的reward的问题
	- ### 20250529
		- 尝试使用别的库进行训练
			- 换个训练脚本仍然会出现突然运动惩罚，其他的reward之后也出现大幅下跌
				- 很奇怪，使用了torch.clamp还是会爆表
				- 有毒啊，我tm都weight为0了还是会爆表？怪哉
				- 原来是CurrTerm的原因
			- 废了，他妈的只学会到某个位置之后就呆住不动了
			- 原版也会发生这种情况
			- 建议action_rate限定在-0.005之内，joint_vel限定在-0.005之内
			- 去掉camera能大量提高训练速度
		- 理想中的工作流
			- bot每日建模确定环境中各个物体的位置
			- 要想获取对应物体，bot先导航至相应位置
			- 然后根据指令、图像和物体位置pick相应物体
			- 然后导航回去
	- ### 20250530
		- 完整训练了36000step,然而并没有学会，中途似乎是有能lift的，然而最终连lift都无法稳定学习
	- ### 20250603
		- 为什么原版lift能训练成功，而我的无法训练成功，是pos的obs太多的原因吗
		- 越来越觉得，单纯的强化学习局限性太大了，必须要像大模型一样，监督学习训练一个底模再用强化学习去微调，那么应该如何设计模型的输出来适应两者呢？
			- 参考deepseek的方案？
		- 将obs砍成只获得yellow cube的pos，顺便修改了reward，环境数量猛增加到160
			- 训练成功了，我怀疑还是多个物体的pos互相干扰导致的
	- ### 20250604
		- 准备测试一下对其他物体能否lift
		- lift的通用输入应该是
			- 物体的pos和终点的pos
			- 至于有多个物体怎么办，我考虑的解决方案是，先pick再通过clip来评估相似性，如果不相似则逆转动作序列
	- ### 20250605
		- 目前已知给定物体位置能将物体抓取，能否不给定物体位置，而使用深度图来完成
		- 使用深度图的好处是，或许能够从多种物体中抓取对应物体？比如有多种饮料，使用深度图+语义分割，或许能抓对东西，如果光用位置，恐怕很难
			- 但是用到camera就训练起来很费劲
	- ### 20250609
		- 下一个目标是，仅利用初始位置和camera的rgb数据以及深度数据能否学会抓取
	- ### 20250610
		- 尝试魔改slam来兼容isaaclab
		- [[isaaclab兼容3dgs]]
	- ### 20250616
		- 如何方便地在isaacsim的standalone脚本来设置环境