- #reinforcement_learning #policybased
- ## PPO的主要改进
	- 好的，我们已经了解了基础的策略梯度方法（REINFORCE）。近端策略优化（Proximal Policy Optimization, PPO）是目前强化学习中最流行和最有效的算法之一，它在基础策略梯度方法上做了几项关键的改进，主要目标是解决**训练不稳定**和**样本效率低**的问题。
	  
	  以下是 PPO 相对于基础策略梯度（如 REINFORCE）的主要改进点：
	  
	  1.  **解决训练不稳定性：限制策略更新幅度 (Constraining Policy Updates)**
	      *   **问题**: 基础策略梯度（包括使用 reward-to-go 和基线的版本）对学习率（步长）非常敏感。如果更新步子太大，策略可能会突然变得很差，导致性能崩溃且难以恢复。这是因为一次糟糕的更新可能会让智能体进入从未探索过的、奖励极低的区域，并且由于采样偏差，很难再采样到好的轨迹来纠正策略。
	      *   **PPO 的解决方案**: PPO 引入了一种机制来限制每次更新时新策略 $\pi_\theta$ 相对于旧策略 $\pi_{\theta_{old}}$的变化幅度。它通过修改目标函数来实现这一点，确保更新不会过于激进。这使得训练过程更加稳定，对学习率的选择不那么敏感。
	      *   **核心机制 - PPO-Clip**: 最常用的 PPO 变体使用**裁剪（Clipping）**技巧。它定义了一个替代的目标函数（Surrogate Objective）：
	          $$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$
	          这里：
	          *   $\hat{\mathbb{E}}_t$ 表示对时间步 $t$ 取经验平均（基于采集的样本）。
	          *   $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ 是**重要性采样权重**，表示新旧策略在状态 $s_t$ 下选择动作 $a_t$ 的概率比。
	          *   $\hat{A}_t$ 是在时间步 $t$ 的**优势函数（Advantage Function）估计**。
	          *   $\epsilon$ 是一个小的超参数（例如 0.1 或 0.2），定义了裁剪范围 $[1-\epsilon, 1+\epsilon]$。
	          *   $\text{clip}(x, min, max)$ 函数将 $x$ 限制在 $[min, max]$ 区间内。
	          *   $\min(\dots, \dots)$ 操作意味着：如果 $r_t(\theta)$ 使得目标函数增加得过多（当 $\hat{A}_t > 0$ 时 $r_t > 1+\epsilon$，或当 $\hat{A}_t < 0$ 时 $r_t < 1-\epsilon$），就使用裁剪后的值，从而限制了单次更新对策略的改变幅度。这就像给策略更新加了一个“软约束”。
	  
	  2.  **提高样本效率：重用旧数据 (Data Reuse / Sample Efficiency)**
	      *   **问题**: 基础的策略梯度是严格的 **on-policy** 算法。这意味着用于计算梯度更新的样本必须是用当前策略 $\pi_\theta$ 收集的。每次更新参数后，之前收集的数据就变得“过时”了，必须丢弃并重新采样。这导致样本效率很低，需要大量的交互才能学习。
	      *   **PPO 的解决方案**: 通过使用**重要性采样**（即概率比 $r_t(\theta)$）和**目标函数裁剪**，PPO 可以在一定程度上安全地使用从旧策略 $\pi_{\theta_{old}}$ 收集的数据来更新当前策略 $\pi_\theta$。裁剪机制保证了即使使用旧数据，策略更新也不会偏离太远，从而避免了重要性采样可能带来的高方差和不稳定问题。这使得 PPO 可以对同一批采集的数据进行**多次（epochs）**梯度更新，显著提高了样本效率。
	  
	  3.  **降低方差：使用优势函数估计 (Advantage Estimation)**
	      *   **问题**: 基础 REINFORCE 使用整个轨迹的总奖励 $R(\tau)$ 或 Reward-to-go $G_t = \sum_{k=t}^T \gamma^{k-t} r_k$ 来评估动作 $a_t$。这会导致很高的方差，因为奖励信号可能非常嘈杂，并且 $G_t$ 的值本身波动很大。
	      *   **PPO 的解决方案**: PPO（以及许多现代策略梯度方法）使用**优势函数（Advantage Function）** $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ 的估计 $\hat{A}_t$ 来代替 $G_t$ 或 $R(\tau)$。优势函数衡量的是在状态 $s_t$ 下选择动作 $a_t$ 相对于平均动作（由价值函数 $V(s_t)$ 体现）有多好。
	          *   使用优势函数作为基线（Baseline）可以显著降低梯度的方差，因为 $A_t$ 的期望通常比 $G_t$ 更接近于零，并且它更直接地反映了动作选择的相对好坏。
	          *   实践中，PPO 通常使用**广义优势估计（Generalized Advantage Estimation, GAE）** 来计算 $\hat{A}_t$，这是一种在偏差和方差之间进行权衡的高级优势估计技术。公式为：
	              $$\hat{A}_t^{GAE(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$
	              其中 $\delta_{t+l} = r_{t+l} + \gamma V(s_{t+l+1}) - V(s_{t+l})$ 是 TD 残差（Temporal Difference error），$\lambda \in [0, 1]$ 是一个平衡因子。
	  
	  **总结 PPO 的改进:**
	  
	  *   **稳定性**: 通过裁剪重要性采样权重，限制策略更新幅度，避免了灾难性的策略崩溃。
	  *   **样本效率**: 通过重要性采样和稳定的更新，可以在同一批数据上进行多次优化迭代，减少了与环境交互的需求。
	  *   **性能**: 结合了优势函数估计（尤其是 GAE），有效降低了梯度方差，使得学习更快、更可靠。
	  *   **实现**: 相对于其前身 TRPO（Trust Region Policy Optimization，另一种限制策略更新的方法），PPO 的实现更简单，计算开销更小，但通常能达到相似或更好的性能。
	  
	  因此，PPO 通过这些精心设计的改进，在保持策略梯度核心思想的同时，显著提升了算法的稳定性、样本效率和整体性能，使其成为许多强化学习任务的有力基准和常用选择。
- ## 主要的技巧
	- ### 裁剪
	- ### **广义优势估计**GAE
		- [[GAE]]
- ## 问题
	- ### 为什么要引入裁剪
		- **直接用 -mb_advantages**: 只能更新一次策略，对于更新后的策略，梯度为零，无法学习。它没有包含当前策略的信息。
		- **需要 ratio**:
			- **引入策略信息**: ratio 依赖于当前策略 
			  
			  ```
			  πθ(at∣st)πθ​(at​∣st​)
			  ```
			  
			  ，因此其梯度不为零，可以驱动策略更新。
		- **重要性采样校正**: 当使用从旧策略收集的数据来更新当前策略时，ratio 是一个重要性采样权重，用于修正分布不匹配的问题。这使得 PPO 可以在一定程度上重用旧数据，提高样本效率。
-