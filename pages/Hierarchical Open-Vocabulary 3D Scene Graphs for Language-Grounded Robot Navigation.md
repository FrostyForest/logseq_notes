- #robotics #robot_navigation #vlm
- # 摘要
  近期的开放词汇机器人建图方法通过预训练的视觉-语言特征丰富了密集几何地图。这些地图虽然能够在针对特定语言概念进行查询时预测逐点显著性图，但在处理大规模环境和超越物体级别的抽象查询时仍面临显著挑战，从而限制了基于语言的机器人导航。在本研究中，我们提出了HOV-SG，一种用于室内机器人导航的分层开放词汇3D场景图建图方法。我们利用开放词汇视觉基础模型，首先在3D中获取最新的开放词汇分割级别地图，随后构建一个包含楼层、房间和物体概念的3D场景图层次结构，并为每个层次添加开放词汇特征。我们的方法能够表示多层建筑，并通过跨楼层的Voronoi图实现机器人遍历。HOV-SG在三个不同数据集上进行了评估，在对象、房间和楼层级别的开放词汇语义精度上超越了现有基线，同时相比密集开放词汇地图实现了75%的表示大小减少。为证明HOV-SG的有效性和泛化能力，我们展示了在真实多层环境中基于语言的长距离机器人导航的成功案例。代码和试验视频数据请访问：
- # 引言
  人类通过多模态体验获取关于世界整体和具体物体的概念性知识。这些语义体验对于物体识别、语言、推理以及规划具有重要意义[1, 2]。认知地图通过传感器融合、分解和层次结构存储这些信息，这在导航物理世界的人类能力中起到核心作用[3, 4, 5]。最近的研究表明，语言作为智能系统与人类之间的有效桥梁，可以帮助机器人在复杂的以人为中心的环境中实现自主性[6-14]。
  传统的机器人导航方法通过同时定位与建图（SLAM）构建高精度的密集空间地图，这支持基于几何目标的精细导航和操控[15]。近期的进展将密集地图与预训练的零样本视觉-语言模型结合，实现了对观测环境的开放词汇索引[9-17]。尽管这些方法将经典机器人技术与现代开放词汇语义结合，但在表示大型场景和实现抽象化方面仍面临显著挑战。基于真实世界感知输入的可扩展场景表示一般应满足以下要求：1）以物体为中心并具备层次化抽象能力；2）在存储容量和可操作性方面高效；3）实现真正的开放词汇索引和便捷查询。
  一些研究通过3D场景图结构实现了对大规模环境的高效表示[18-20]，这些结构同时成为提示大型语言模型（LLM）的有用语义接口。然而，大多数方法依赖于封闭语义集，除了专注于小规模场景的ConceptGraphs [14]。在本研究中，我们提出了分层开放词汇3D场景图（Hierarchical Open-Vocabulary 3D Scene Graphs，简称HOV-SG）。
  我们的方法从密集开放词汇地图中抽象出三个不同概念层次，即楼层、房间和物体。我们在所有概念层次中使用开放词汇视觉-语言模型[21, 22]，构建跨多层环境的3D场景图层次结构，同时保持较小的内存占用。由于其以概念为中心的特性，HOV-SG的表示能够被LLM通过提示有效使用。
  与以往研究不同，我们的方法首先将抽象查询（例如“上层楼浴室中的毛巾”）分解，并将获得的语义标记与层次结构的不同层次进行匹配和评分。此外，我们还引入了一个覆盖多楼层（包括楼梯）的导航Voronoi图，从而将分解的查询有效地与环境关联。这使得从抽象查询中实现物体检索和大规模室内环境中的长距离机器人导航成为可能，如图1所示。
  总结来说，我们的贡献如下：
  1）我们提出了一种基于零样本嵌入特征聚类的新型融合方案，在开放集3D语义分割中达到了当前最先进的效果。
  2）我们设计了一种算法，用于构建真正可操作的多楼层建筑的开放词汇3D场景图。
  3）我们在Replica[23]和ScanNet[24]数据集上评估了我们方法的语义分割性能，并在Habitat-Matterport 3D Semantics数据集[25]上分析了场景图的关键属性。此外，我们还进行了详细的消融研究以证明设计选择的合理性。
  4）我们基于自然语言的长查询，在真实多楼层环境中进行了物体导航实验。
  5）我们引入了一种新的评估指标，称为AUCtop-k，用于衡量开放词汇语义的表现。
  6）我们将代码和评估协议公开于：
- # 相关工作
	- ## A. 语义3D建图
	  通过语义信息丰富几何地图是实现灵活多功能导航系统的基础[26, 27, 16, 10, 4, 9]。过去的研究通过以下方法创建了增强语义或实例级的地图：
	  学习传感器观测特征[28]；
	  将预先构建的物体形状匹配到几何地图[29]；
	  将2D语义预测反投影到3D空间[30, 31, 32]；
	  用基本3D元素（如立方体或二次曲面）实例化2D检测[33, 34]。
	  这些方法展示了重建具有精确几何结构和语义含义场景的能力。然而，大多数方法依赖于固定的类别集，受限于训练的语义预测模型或预定义的相关物体原型集合。
	  
	  随着大型视觉-语言模型（如CLIP[21]）及其微调版本的进步，一些研究提出了将视觉-语言特征集成到几何地图中的表示方法，从而实现对象[11, 16, 10, 13, 35, 17, 12]、音频数据[16, 13]和图像[26, 16]在非结构化环境中的开放词汇索引。这些方法突破了固定语义类别的限制，但通常需要为地图中的每个几何元素（如点、体素或2D单元格）存储视觉-语言嵌入，导致存储开销显著增加。
	- ## B. 3D场景图
	  
	  3D场景图作为一种以物体为中心的表示形式，在大规模室内[36, 19, 20]和室外场景[18]中表现出色。通过将物体或空间概念表示为节点，将它们的关系表示为边，3D场景图能够高效地表示大型场景[36, 18, 19]。节点和边可以包含几何和语义属性，通常通过现成的网络推断[37]。
	  
	  通过将场景分解为物体及其关系，3D场景图支持机器人导航和操控所需的高层次推理。这在推理、规划和导航领域尤为重要，因为这些任务以物体为中心[19, 38]。结合来自同时定位与建图（SLAM）的里程计估计[39, 18, 40]，3D场景图还能实现语义与高精度建图方法（如网格）之间的紧密结合[19]。
	  
	  早期研究展示了如何通过空间和语义领域的抽象编码层次结构，通常采用离线方法[36, 20]。后续研究探讨了基于学习的场景图构建[41, 37]以及动态室内场景[19]。一些方法（如SceneGraphFusion[37]和S-Graphs[39]）还研究了其提出方法的实时能力。最近，ConceptGraphs[14]首次展示了如何将3D场景图与开放词汇视觉-语言特征结合。此外，该研究还展示了如何使用大型语言模型（LLM）查询图，并实现多种下游应用。
	- ## C. 用于规划的场景图
	  
	  近年来，一些研究探索了将场景图用于机器人规划的可能性。最早的方法依赖预探索的环境，并通过迭代场景图分解来获取有基础的计划[38, 42]。例如，RobLM[42]通过一个微调的GPT-2实例分解规划阶段，提出场景图中的高层次子问题，并通过PDDL任务规划器解决。SayPlan[38]直接利用GPT-4[43]对场景图进行迭代搜索，以生成具有可操作性约束的计划。
	  
	  另一条研究路线探索了基于场景图的机器人导航。SayNav[44]从场景图中获取LLM生成的计划，并执行短距离点目标导航子任务。而VoroNav[45]则构建了一个基于相机观测的Voronoi图，以解决物体导航任务。与此同时，MoMa-LLM[46]使用场景图和GPT-4处理特定任务的移动操控目标。类似地，GRID[47]通过图神经网络预测场景图和LLM编码的操作。
	- ## 本研究的贡献
	  
	  概念上，我们的工作与ConceptGraphs和Hydra最为相似。ConceptGraphs主要在小规模场景中进行评估，并通过人类评估其节点语义精度等属性。而我们的工作不仅提出了一种用于衡量对象特征语义精度的新指标，还引入了开放词汇层次结构。与ConceptGraphs不同，也不同于Hydra[19]（未处理开放词汇特征），我们的方法展示了如何高效地表示具有开放词汇特征的可操作、分层3D场景图。
- # 技术路线
	- 本研究旨在通过RGB-D观测和里程计数据，为大规模多楼层室内环境开发一种简洁高效的视觉-语言图表示方法。该图能够通过自然语言查询实现多层次语义概念的索引，例如“第一层楼”（楼层层次）、“第一层楼的办公室”（房间层次）、以及“第二层楼办公室里的植物”（物体层次）。此外，该图还应具备可操作性，使机器人无需额外的几何地图即可在语义和空间上实现环境的定位和导航。
	  为实现这一目标，我们提出了一种 **分层开放词汇场景图**（Hierarchical Open-Vocabulary Scene Graph，简称HOV-SG）。整个流程包括两个阶段：
	  构建一个3D分段级开放词汇地图。
	  基于该地图创建分层开放词汇场景图。
	  接下来的部分将详细描述：(i) 3D分段级开放词汇地图的构建（第III-A节），(ii) 分层开放词汇场景图的创建（第III-B节），以及(iii) 如何使用该图实现大规模环境中的语言条件导航（第III-C节）。图2展示了我们方法的整体概览。
	- ## A. 3D分段级开放词汇建图
	  collapsed:: true
		- 构建分段级开放词汇地图的核心思想是从RGB-D视频和里程计数据中生成一系列3D点云（即分段），并为每个分段分配由预训练视觉-语言模型（VLM）生成的开放词汇特征。与以往为每个3D点单独分配视觉-语言特征的方法不同[11, 10, 13, 12]，我们利用了3D空间中相邻点通常共享相同语义信息的特点。这意味着在保持表达能力的同时，可以减少所需的语义特征数量。
		- ### 帧级3D分段合并
		  
		  给定一组RGB-D观测序列，我们利用Segment Anything[22]为每一时刻生成一系列无类别的2D二值掩码。掩码中的像素随后使用深度信息反投影到3D空间，生成一组点云（即3D分段）。基于精确的里程计估计，我们将所有3D分段转换到全局坐标系中。
		  
		  这些帧级分段通过以下方式初始化为新的全局分段或与已有分段合并：
		  $$R(m,n)=\max(\text{overlap}(S_m, S_n), \text{overlap}(S_n, S_m))$$
		  其中，$$S_m$$和 $$S_n$$ 分别表示分段 m 和 n，$$\text{overlap}(S_a, S_b)$$ 表示分段 $$S_a$$中的点在 $$S_b$$ 中邻居点的数量与 $$S_a$$ 总点数的比值。
		  
		  不同于Gu等[14]通过增量方式将新分段与具有最大重叠比的全局分段合并，我们构建了一个完全连接的图，其中每个分段作为节点，边的权重为对应的重叠比。根据这些权重，将高度连接的子图合并。这种方式允许一个分段与多个分段合并，例如当一个新分段填补了两个已注册全局分段之间的间隙时尤其有用。
		- ### 分段级开放词汇特征计算
		  
		  对于每一帧生成的2D SAM掩码，我们根据其边界框裁剪出目标图像，并生成去除背景的掩码图像。然后，我们分别使用CLIP [21]对完整RGB帧及上述两种掩码图像进行编码，并通过加权求和的方式融合这些特征（图2左侧）。
		  
		  此前的研究[48]提出使用去除背景后的掩码图像的CLIP特征，而另一些研究[13]则通过结合全图像的CLIP特征和目标掩码裁剪区域的CLIP特征（包含背景）来计算特征。在我们的工作中，我们实验证明同时对完整RGB帧和两种掩码图像进行CLIP编码并加权融合能够实现更好的结果（详见第IV-E节）。
		  
		  融合公式如下：
		  
		  $$f_i = w_g f_g + w_l f_l + w_m f_m$$
		  
		  其中，$$f_i$$ 表示帧中第 ii 个2D掩码的融合特征；$$f_g$$、$$f_l$$ 和 $$f_m$$ 分别表示从完整RGB帧、掩码裁剪图像（包含背景）以及掩码裁剪图像（不包含背景）中提取的CLIP特征；$$w_g$$、$$w_l$$ 和 $$w_m$$ 为对应的权重，其总和为1。
		  
		  在为每个掩码生成单个CLIP特征后，我们将2D掩码转换到全局3D坐标系，并将融合后的CLIP特征与预先计算的参考点云中最接近的3D点关联起来。基于这种关联，我们将获得的分段特征注册到全局点特征图中。最终的点特征通过计算每个参考点关联特征的平均值确定。
		  
		  基于独立合并步骤获得的3D分段，我们可以推断出所有3D分段的开放词汇视觉-语言特征，如图2所示。在随后的步骤中，我们将点特征与获得的3D分段匹配。对于分段中的每个点，我们识别其在参考点云中的最近邻点，并收集其CLIP特征。
		  
		  随后，我们利用DBSCAN对分段的所有点特征进行聚类，并将与主簇均值最接近的特征分配给该分段（图2中间部分）。这种方法避免了模式坍缩，同时消除了噪声，从而生成更具语义意义的分段特征。
	- ## B. 3D 场景图构建
	  collapsed:: true
	  
	  本节描述如何利用场景的全局参考点云、全局 3D 分段列表及其关联的 CLIP 特征（见第 III-A 节）构建分层的开放词汇场景图。我们将图形式化为 G = (N, E)，其中 N 表示节点，E 表示边。
		- ### **节点表示**
		  
		  图的节点 NN 包括以下四种类型：
		  
		  $$N = N_S \cup N_F \cup N_R \cup N_O$$
		  N_S: 根节点，表示整个场景。
		  N_F: 楼层节点，对应环境中的每一层楼。
		  N_R: 房间节点，表示楼层上的每个房间。
		  N_O: 对象节点，表示房间内检测到的对象。
		  除根节点 N_S 外，每个节点包含：
		  所表示概念对应的点云。
		  相关的开放词汇 CLIP 特征。
		- ### **边表示**
		  
		  图中的边 E 表示层次关系，定义如下：
		  
		  $$E = E_{SF} \cup E_{FR} \cup E_{RO}$$
		  E_{SF}: 根节点与楼层节点之间的边。
		  E_{FR}: 楼层节点与房间节点之间的边。
		  E_{RO}: 房间节点与对象节点之间的边。
		- ### **楼层分割**
		  
		  为了区分楼层，我们在点云中识别高度直方图的峰值：
		  使用 0.01 米的箱宽构建点云在高度轴上的直方图。
		  在直方图中（局部范围为 0.2 米）识别峰值，仅保留超过最高强度峰值 90% 的峰。
		  应用 DBSCAN 聚类算法，并为每个聚类选择排名前两位的峰值。
		  将排序后的高度向量中每两个连续的值表示为建筑物的一个楼层（地板和天花板）。
		  楼层分割过程如图 3 所示。根据得到的高度值，可以提取每层楼的点云 PlP_l，其中 ll 是楼层编号。此外，为每个楼层节点附加一个 CLIP 文本嵌入，模板为“floor {}”。在根节点和每个楼层节点 (NS, Nl) ∈ ESF 之间建立图形边。
		- ### 房间分割
		  基于每个获得的楼层点云，我们构建一个 2D 鸟瞰图 (BEV) 直方图，通过对直方图进行阈值处理，从中提取二值墙骨架掩码。在扩张墙掩码并计算欧几里得距离场 (EDF) 后，通过对 EDF 进行阈值处理可以得出多个孤立区域。将这些区域作为种子，我们应用分水岭算法来获得 2D 区域掩码。房间分割过程进一步如图 3 所示。给定 2D 区域掩码，我们提取落入楼层高度区间以及 BEV 房间段的 3D 点，以形成房间点云，这些点云稍后用于将对象关联到房间。
		- 为了用开放词汇特征丰富房间节点，我们将相机姿态位于房间分割内的 RGB-D 观测值与这些房间相关联（参见图 4）。通过使用 k-means 算法提取 k 个代表性视图嵌入来提炼这些图像的 CLIP 嵌入。在推理过程中，给定一个通过 CLIP 编码的房间类别列表，我们构建一个 k 个代表性特征和所有房间类别特征之间的余弦相似度矩阵。接下来，我们沿着类别轴取最大值（argmax），并分别获得每个代表最可能的房间类型，从而为每个房间产生 k 个投票。根据这些投票，我们通过取每个房间所有 k 个代表中的最高分投票或多数投票来获得预测的房间类别。这 k 个代表性嵌入和房间点云为楼层 f 上的房间 r 赋予房间节点 Nf,r 属性。在楼层节点和每个房间节点之间建立一条边 (Nf, Nf,r) ∈ EFR。图 4 说明了视图嵌入的构建和查询。
		- ### 物体识别
		  给定房间点云，我们将那些在鸟瞰视图中与潜在候选房间点云有重叠的物体级3D分段关联起来。每当一个分段与任何房间都没有重叠时，我们将其关联到与之距离最小的房间。为了减少节点数量，我们合并了那些在成对部分重叠显著的情况下（见第三节A部分），在查询选定的标签集时产生相同物体标签的3D分段。每个合并后的点云构成一个物体节点Nf,r,o，该节点通过边(Nf,r, Nf,r,o) ∈ ERO连接到其对应的房间节点Nf,r ∈ NR。每个物体节点包含其相应的3D分段特征（如第三节A部分所述）、3D分段点云以及一个用于临时命名的最高分物体标签。
		- ### 可操作图的创建
		  除了开放词汇层次结构外，场景图还包含一个导航Voronoi图，用于机器人在已映射环境中的可穿越性，跨越多个楼层[49]。这使得基于Voronoi图的高层规划和低层执行成为可能。可操作图的创建包括构建每层楼和跨楼层的导航图。对于楼层级的图，方法包括计算楼层的自由空间图，并基于此创建Voronoi图[49]。为了构建每层楼的图，我们首先获取所有相机姿态，并将其作为二维点投影到每层楼的俯视图（BEV）上，假设两个节点之间一定半径内的区域是成对可导航的。随后，通过将所有楼层点的投影到BEV平面来获取整个楼层区域。根据在预定高度范围内[ymin + δ1, ymin + δ2]的点生成障碍图，其中ymin为楼层点的最小高度，而δ1 = 0.2，δ2 = 1.5是经验调整的值。通过将姿态区域图和楼层区域图的并集减去障碍区域图，得出每层楼的自由空间图。此自由图的Voronoi图即为楼层图（见图5a）。为了构建跨楼层导航图，将楼梯上的相机姿态连接起来形成楼梯级图。随后，选择楼梯图与楼层图之间最接近的节点并进行连接，从而完成跨楼层导航图的构建，如图5b所示。
	- ## C. 场景图导航
		- HOV-SG 将潜在导航目标的范围扩展到比简单的物体目标[14, 16, 10, 13]更具体的空间概念，如区域和楼层。使用HOV-SG进行语言引导的导航涉及处理复杂的查询，如“在2楼的浴室中找到厕所”，这需要使用大语言模型（提示在补充材料S.1-A节中给出）。我们将这种冗长的指令分解为三个独立的查询：分别针对楼层级、房间级和物体级。
		- 利用HOV-SG的明确层次结构，我们顺序地针对每个层次进行查询，以逐步缩小解决方案的范围。这是通过计算查询楼层、查询区域和查询物体与图中给定的所有物体、房间和楼层的余弦相似度来实现的。一旦通过评分确定了目标节点，我们就利用前面提到的导航图来规划从起始姿态到目标目的地的路径，这在图S.1中演示，并在图1中可视化。
-