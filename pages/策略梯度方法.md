- #reinforcement_learning #策略梯度
- 基于策略的强化学习的目标是学习最优的动作策略使得每个轨迹的期望奖励最大，奖励的期望具体表达如下：
- $$\bar{R}_\theta=∑_τ​R(τ)p_θ​(τ)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$$
- 因为要让奖励的期望越大越好，所以我们要对奖励的期望使用[[梯度上升]]的方法进行优化，要使用梯度上升，就要首先计算奖励期望$\bar{R}_\theta$对模型参数$\theta$的梯度，即求$\bar{R}$对$\theta$的导数：
- $$\nabla \bar{R}_{\theta} = \sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)$$
- 其中$R(\tau)$为奖励函数，与动作模型无关，动作模型所能决定的仅仅是各个轨迹出现的概率。
- 利用一些技巧可以把梯度也写成关于轨迹$\tau$的期望的形式：
- $$\begin{aligned}
  \nabla \bar{R}_{\theta} &= \sum_{\tau} R(\tau) \nabla p_{\theta}(\tau) \\
  &= \sum_{\tau} R(\tau)p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\
  &= \sum_{\tau} R(\tau)p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
  &= \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla \log p_{\theta}(\tau)]
  \end{aligned}$$
- 实际上期望值无法计算，所以我们用采样即蒙特卡洛方法采样多个轨迹并求平均即可得到梯度：
- $$\mathbb{E}_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla \log p_{\theta}(\tau)] \approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla \log p_{\theta}(\tau^n)$$
- 给定动作策略的参数 $\theta$，基于马尔可夫性质，我们可以计算某个轨迹$\tau$发生的概率为：
- $$\begin{aligned}
  p_{\theta}(\tau) &= p(s_1) p_{\theta}(a_1|s_1) p(s_2|s_1, a_1) p_{\theta}(a_2|s_2) p(s_3|s_2, a_2) \cdots \\
  &= p(s_1) \prod_{t=1}^{T} p_{\theta}(a_t|s_t) p(s_{t+1}|s_t, a_t)
  \end{aligned}$$
- $R(\tau)$由环境给出，代入轨迹概率公式，$\nabla\log p_{\theta}(\tau^n)$计算方法为：
- $$\begin{aligned}
  \nabla \log p_{\theta}(\tau) &= \nabla \left( \log p(s_1) + \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) + \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t) \right) \\
  &= \nabla \log p(s_1) + \nabla \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) + \nabla \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t) \\
  &= \nabla \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) \\
  &= \sum_{t=1}^{T} \nabla \log p_{\theta}(a_t|s_t)
  \end{aligned}$$
- 综上所述，策略梯度的计算公式为：
- $$\nabla \bar{R}_{\theta} = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla \log p_{\theta}(a_t^n|s_t^n)$$
- 新参数$\theta$的计算公式为:
	- $$\theta_{new}=\theta_{old}+\alpha*\nabla_{\theta} \bar{R}(\theta) $$
- $\nabla \log p_{\theta}(a_t^n|s_t^n)$又如何计算呢
	- $p_{\theta}(a_t^n|s_t^n)$的计算方法为，模型观察环境并针对各个动作输出一个值，再通过Softmax函数转为概率。具体的$$\log p_{\theta}(a_t^n|s_t^n)$$对参数$$\theta$$的梯度就是由pytorch框架的自动微分来得到结果。
- ## 存在的问题
	- *   **高方差问题**: 这个基础的 REINFORCE 公式（使用整个轨迹的总奖励 $R(\tau^n)$ 来“评价”在时间步 $t$ 采取的动作 $a_t^n$）存在高方差问题。实践中通常会进行改进，例如：
	      *   **Reward-to-go**: 用从时间步 $t$ 开始的未来奖励之和 $G_t^n = \sum_{k=t}^{T_n} r(s_k^n, a_k^n)$ 代替 $R(\tau^n)$。因为 $t$ 时刻的动作不影响 $t$ 之前的奖励。
	      *   **基线 (Baseline)**: 从奖励项中减去一个不依赖于动作 $a_t$ 但可以依赖于状态 $s_t$ 的基线 $b(s_t)$，例如状态价值函数 $V(s_t)$ 的估计。这可以减小方差而不改变梯度的期望值。修改后的项变为 $(G_t^n - b(s_t^n)) \nabla \log p_{\theta}(a_t^n|s_t^n)$。
		- 原理：$$Var(X-Y)=Var(x)+Var(y)-2*Cov(X,Y)$$
	- *   这些改进是为了提高学习效率和稳定性，但并不意味着原始的 REINFORCE 推导是错误的。你提供的推导准确地反映了 REINFORCE 算法的理论基础。
	- ### 如果不引入baseline且reward恒为正，那梯度岂不是一直是正数？
	  collapsed:: true
		- 你说得非常对！这是一个非常好的观察，也是**引入基线（Baseline）的一个关键动机**。
		  
		  如果**不引入基线**，并且**回报 `G_t^n`（或者单步奖励 `r_t`，如果只考虑单步）总是正数**，那么策略梯度的估计项 `G_t^n * ∇ log p_θ(a_t^n | s_t^n)` 会出现以下情况：
		  
		  1.  **`G_t^n > 0`**: 因为回报总是正的。
		  2.  **`∇ log p_θ(a_t^n | s_t^n)`**: 这个向量指向参数空间中能够**增加**在状态 `s_t^n` 下采取**实际采取的动作** `a_t^n` 的概率 `p_θ(a_t^n | s_t^n)` 的方向。
		  
		  因此，两者的乘积 `G_t^n * ∇ log p_θ(a_t^n | s_t^n)` 的方向将总是指向**增加刚刚执行过的动作 `a_t^n` 的概率**的方向。
		  
		  **这会导致什么问题？**
		  
		  *   **所有动作都被“鼓励”：** 无论在某个状态下采取的动作是好是坏（只要它产生的回报是正数，哪怕只是一个很小的正数），它的概率都会被提升。
		  *   **相对好坏被忽略：** 假设在状态 `s` 下有两个动作 `a1` 和 `a2`。采取 `a1` 平均回报是 +100，采取 `a2` 平均回报是 +1。如果不使用基线，当你采样到 `a2` 并获得 +1 的回报时，你仍然会增加 `a2` 的概率。虽然当你采样到 `a1` 时，你会更大幅度地增加 `a1` 的概率，但对 `a2` 的“错误鼓励”会减慢学习过程，增加学习所需样本量。
		  *   **高方差：** 梯度估计的方差会很大。因为回报的大小（即使都是正数）可能波动很大，导致梯度更新的步长和方向不稳定。
		  
		  **基线如何解决这个问题？**
		  
		  通过引入基线 `b(s_t^n)`（通常是状态价值函数 `V(s_t^n)` 的估计），我们将乘数项从 `G_t^n` 变为 **`A_t^n = G_t^n - b(s_t^n)`**，这被称为**优势函数（Advantage Function）**。
		  
		  *   **如果 `G_t^n > b(s_t^n)`** (即 `A_t^n > 0`)：说明这次实际获得的回报比该状态下的平均预期回报要**好**。这时 `A_t^n` 是正数，梯度更新会**增加**动作 `a_t^n` 的概率。
		  *   **如果 `G_t^n < b(s_t^n)`** (即 `A_t^n < 0`)：说明这次实际获得的回报比该状态下的平均预期回报要**差**（即使 `G_t^n` 本身是正数）。这时 `A_t^n` 是负数，梯度更新实际上会**减少**动作 `a_t^n` 的概率。
		  
		  **总结：**
		  
		  是的，如果没有基线且奖励恒为正，理论上单次梯度更新总是会“鼓励”当前执行的动作。基线通过将绝对回报转换为**相对回报（优势）**，使得只有**表现优于平均水平**的动作才会被鼓励，表现差于平均水平的动作会被抑制，从而大大提高了学习的稳定性和效率，并降低了方差。这就是基线在策略梯度方法中如此重要的原因。
-
-