- #reinforcement_learning #策略梯度
- 强化学习的目标是使得每个轨迹的奖励最大，奖励的期望具体表达如下：
- $$\bar{R}_\theta=∑_τ​R(τ)p_θ​(τ)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$$
- 因为要让奖励的期望越大越好，所以我们要对奖励的期望使用[[梯度上升]]的方法进行优化，要使用梯度上升，就要首先计算奖励期望$\bar{R}_\theta$对模型参数$\theta$的梯度，即求$\bar{R}$对$\theta$的导数：
- $$\nabla \bar{R}_{\theta} = \sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)$$
- 其中$R(\tau)$为奖励函数，与动作模型无关，动作模型所能决定的仅仅是各个轨迹出现的概率。
- 利用一些技巧可以把梯度也写成关于轨迹$\tau$的期望的形式：
- $$\begin{aligned}
  \nabla \bar{R}_{\theta} &= \sum_{\tau} R(\tau) \nabla p_{\theta}(\tau) \\
  &= \sum_{\tau} R(\tau)p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\
  &= \sum_{\tau} R(\tau)p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
  &= \mathbb{E}_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla \log p_{\theta}(\tau)]
  \end{aligned}$$
- 实际上期望值无法计算，所以我们用采样的方式采样多个轨迹并求平均即可得到梯度：
- $$\mathbb{E}_{\tau \sim p_{\theta}(\tau)} [R(\tau) \nabla \log p_{\theta}(\tau)] \approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla \log p_{\theta}(\tau^n)$$
- 给定演员的参数 $\theta$，我们可以计算某个轨迹$\tau$发生的概率为：
- $$\begin{aligned}
  p_{\theta}(\tau) &= p(s_1) p_{\theta}(a_1|s_1) p(s_2|s_1, a_1) p_{\theta}(a_2|s_2) p(s_3|s_2, a_2) \cdots \\
  &= p(s_1) \prod_{t=1}^{T} p_{\theta}(a_t|s_t) p(s_{t+1}|s_t, a_t)
  \end{aligned}$$
- $R(\tau)$由环境给出，代入轨迹概率公式，$\nabla\log p_{\theta}(\tau^n)$计算方法为：
- $$\begin{aligned}
  \nabla \log p_{\theta}(\tau) &= \nabla \left( \log p(s_1) + \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) + \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t) \right) \\
  &= \nabla \log p(s_1) + \nabla \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) + \nabla \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t) \\
  &= \nabla \sum_{t=1}^{T} \log p_{\theta}(a_t|s_t) \\
  &= \sum_{t=1}^{T} \nabla \log p_{\theta}(a_t|s_t)
  \end{aligned}$$
- 综上所述，策略梯度的计算公式为：
- $$\nabla \bar{R}_{\theta} = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla \log p_{\theta}(a_t^n|s_t^n)$$
- 新参数$\theta$的计算公式为:
	- $$\theta_{new}=\theta_{old}+\alpha*\nabla_{\theta} \bar{R}(\theta) $$
- $\nabla \log p_{\theta}(a_t^n|s_t^n)$又如何计算呢
	- $p_{\theta}(a_t^n|s_t^n)$的计算方法为，模型观察环境并针对各个动作输出一个值，再通过Softmax函数转为概率。具体的梯度就是由pytorch框架的自动微分来得到结果。
-
-
-
-
-
-